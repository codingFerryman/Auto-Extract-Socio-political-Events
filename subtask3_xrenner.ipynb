{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "pre-trained eng_flair_nner_distilbert.pt\n",
    "\n",
    "demo: https://corpling.uis.georgetown.edu/xrenner/\n",
    "\n",
    "Model: https://corpling.uis.georgetown.edu/amir/download/\n",
    "\n",
    "Code: https://github.com/amir-zeldes/xrenner\n",
    "\n",
    "Doc: https://corpling.uis.georgetown.edu/xrenner/doc/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.2.1.json:   0%|   â€¦",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "574c017c561043cb9cd68e0fd88ed101"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-24 00:30:16 INFO: Downloading default packages for language: en (English)...\n",
      "2021-06-24 00:30:18 INFO: File exists: /home/he/stanza_resources/en/default.zip.\n",
      "2021-06-24 00:30:28 INFO: Finished downloading models and saved to /home/he/stanza_resources.\n",
      "2021-06-24 00:30:28 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| pos       | combined  |\n",
      "| lemma     | combined  |\n",
      "| depparse  | combined  |\n",
      "| sentiment | sstplus   |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2021-06-24 00:30:28 INFO: Use device: gpu\n",
      "2021-06-24 00:30:28 INFO: Loading: tokenize\n",
      "2021-06-24 00:30:30 INFO: Loading: pos\n",
      "2021-06-24 00:30:30 INFO: Loading: lemma\n",
      "2021-06-24 00:30:30 INFO: Loading: depparse\n",
      "2021-06-24 00:30:31 INFO: Loading: sentiment\n",
      "2021-06-24 00:30:32 INFO: Loading: ner\n",
      "2021-06-24 00:30:32 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# pre-trained eng_flair_nner_distilbert.pt\n",
    "# Demo: https://corpling.uis.georgetown.edu/xrenner/\n",
    "# Model: https://corpling.uis.georgetown.edu/amir/download/\n",
    "# Code: https://github.com/amir-zeldes/xrenner\n",
    "# Doc: https://corpling.uis.georgetown.edu/xrenner/doc/\n",
    "\n",
    "import json\n",
    "import random\n",
    "\n",
    "import stanza\n",
    "from stanza.utils.conll import CoNLL\n",
    "\n",
    "from xrenner import Xrenner\n",
    "\n",
    "stanza.download('en')\n",
    "nlp_stanza = stanza.Pipeline('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "with open(\"./data/subtask3-coreference/en-train.json\") as fp:\n",
    "    entries = fp.readlines()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "data = []\n",
    "for entry in entries:\n",
    "    data.append(json.loads(entry))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples in the dataset:\n"
     ]
    },
    {
     "data": {
      "text/plain": "[{'event_clusters': [[6], [1]],\n  'sentence_no': [1, 6],\n  'sentences': ['Several hundred people who have occupied homes at Delft on the Cape Flats are on their way to the Cape High Court in a bid to block their impending eviction , a spokesman said .',\n   'Speaking just after 1pm , Poni said he and the hundreds of residents who had demonstrated outside the court were now on their way by train to the High Court , where they would make another attempt to get an urgent interdict .'],\n  'id': 55550},\n {'event_clusters': [[3, 4], [1, 7]],\n  'sentence_no': [1, 3, 4, 7],\n  'sentences': ['Flash strike by guards delays suburban trains - Indian Express Express News Service , Express News Service : Fri Nov 30 2012 , 03:16 hrs Most guards on local trains abstained from work Thursday protesting \" irritable \" behaviour of the area officer at Churchgate , forcing station masters and traffic inspectors to take up their role .',\n   'Four days ago , guard Rakesh Jha had gone on a hunger strike at Churchgate Station after he was denied leave despite applying two months back .',\n   'A guard said , \" When Jha did not get leave till four hours before departure of his train , he hanged a poster around his neck announcing an indefinite hunger strike .',\n   'All the 417 guards in the suburban section refused to work extra hours in protest .'],\n  'id': 55412}]"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Examples in the dataset:\")\n",
    "random.sample(data, 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries\n"
     ]
    },
    {
     "data": {
      "text/plain": "596"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of entries\")\n",
    "len(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "sentence_no_list = []\n",
    "event_clusters_list = []\n",
    "\n",
    "for d in data:\n",
    "    sentence_no_list += d['sentence_no']\n",
    "    event_clusters_list += d['event_clusters']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Label data\n",
    "for data_entry in data:\n",
    "    _id = data_entry['id']\n",
    "    _docu = data_entry['sentences']\n",
    "    _docu_in = [stanza.Document([],text=d) for d in _docu]\n",
    "    _docu_out = nlp_stanza(_docu_in)\n",
    "    data_entry['stanza'] = _docu_out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# Convert Stanza results to CoNLL10 format\n",
    "for data_entry in data:\n",
    "    data_entry['conll'] = [CoNLL.convert_dict(s.to_dict()) for s in data_entry['stanza']]\n",
    "    for ec in data_entry['conll']:\n",
    "        _ec = ec[0]\n",
    "        _tmp = []\n",
    "        for _ecc in _ec:\n",
    "            _tmp.append('\\t'.join(_ecc))\n",
    "        data_entry['conll_text'] = '\\n'.join(_tmp)+'\\n\\n'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "[{\n   \"text\": \"Pakistan\",\n   \"type\": \"GPE\",\n   \"start_char\": 10,\n   \"end_char\": 18\n },\n {\n   \"text\": \"along border October 12\",\n   \"type\": \"EVENT\",\n   \"start_char\": 54,\n   \"end_char\": 77\n },\n {\n   \"text\": \"2014\",\n   \"type\": \"DATE\",\n   \"start_char\": 80,\n   \"end_char\": 84\n },\n {\n   \"text\": \"Parishad\",\n   \"type\": \"GPE\",\n   \"start_char\": 187,\n   \"end_char\": 195\n },\n {\n   \"text\": \"Saturday\",\n   \"type\": \"DATE\",\n   \"start_char\": 208,\n   \"end_char\": 216\n },\n {\n   \"text\": \"Pakistan\",\n   \"type\": \"GPE\",\n   \"start_char\": 280,\n   \"end_char\": 288\n }]"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_stanza(data[0]['sentences'][0]).entities"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "[{\n   \"text\": \"Union\",\n   \"type\": \"ORG\",\n   \"start_char\": 41,\n   \"end_char\": 46\n }]"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_stanza(data[0]['sentences'][1]).entities\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/he/anaconda3/envs/xrenner/lib/python3.6/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator LabelEncoder from version 0.22.1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/he/anaconda3/envs/xrenner/lib/python3.6/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator OrdinalEncoder from version 0.22.1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "xrenner = Xrenner()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BasicTokenizer' object has no attribute 'strip_accents'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-27-c90b85e17d88>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mxrenner\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0manalyze\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'conll_text'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mout_format\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'conll'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/anaconda3/envs/xrenner/lib/python3.6/site-packages/xrenner/modules/xrenner_xrenner.py\u001B[0m in \u001B[0;36manalyze\u001B[0;34m(self, infile, out_format)\u001B[0m\n\u001B[1;32m    161\u001B[0m                                 \u001B[0mseq_preds\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlex\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msequencer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict_proba\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    162\u001B[0m                         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 163\u001B[0;31m                                 \u001B[0mseq_preds\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlex\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msequencer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict_proba\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ms_texts\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    164\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    165\u001B[0m                 \u001B[0;32mfor\u001B[0m \u001B[0mmyline\u001B[0m \u001B[0;32min\u001B[0m \u001B[0minfile\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/xrenner/lib/python3.6/site-packages/xrenner/modules/xrenner_sequence.py\u001B[0m in \u001B[0;36mpredict_proba\u001B[0;34m(self, sentences)\u001B[0m\n\u001B[1;32m    302\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmajor\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mminor\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m4\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    303\u001B[0m                 \u001B[0msentences\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mSentence\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ms\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0muse_tokenizer\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mq\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mq\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0ms\u001B[0m \u001B[0;32min\u001B[0m \u001B[0msentences\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 304\u001B[0;31m             \u001B[0mpreds\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtagger\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msentences\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    305\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    306\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mpreds\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# Newer versions of flair have void predict method, use modified Sentence list\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/xrenner/lib/python3.6/site-packages/flair/models/sequence_tagger_model.py\u001B[0m in \u001B[0;36mpredict\u001B[0;34m(self, sentences, mini_batch_size, all_tag_prob, verbose, label_name, return_loss, embedding_storage_mode)\u001B[0m\n\u001B[1;32m    367\u001B[0m                     \u001B[0;32mcontinue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    368\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 369\u001B[0;31m                 \u001B[0mfeature\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbatch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    370\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    371\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mreturn_loss\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/xrenner/lib/python3.6/site-packages/flair/models/sequence_tagger_model.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, sentences)\u001B[0m\n\u001B[1;32m    606\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msentences\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mList\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mSentence\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    607\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 608\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0membeddings\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0membed\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msentences\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    609\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    610\u001B[0m         \u001B[0mnames\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0membeddings\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_names\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/xrenner/lib/python3.6/site-packages/flair/embeddings/token.py\u001B[0m in \u001B[0;36membed\u001B[0;34m(self, sentences, static_embeddings)\u001B[0m\n\u001B[1;32m     69\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     70\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0membedding\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0membeddings\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 71\u001B[0;31m             \u001B[0membedding\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0membed\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msentences\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     72\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     73\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0mproperty\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/xrenner/lib/python3.6/site-packages/flair/embeddings/base.py\u001B[0m in \u001B[0;36membed\u001B[0;34m(self, sentences)\u001B[0m\n\u001B[1;32m     58\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     59\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0meverything_embedded\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstatic_embeddings\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 60\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_add_embeddings_internal\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msentences\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     61\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     62\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0msentences\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/xrenner/lib/python3.6/site-packages/flair/embeddings/legacy.py\u001B[0m in \u001B[0;36m_add_embeddings_internal\u001B[0;34m(self, sentences)\u001B[0m\n\u001B[1;32m   1195\u001B[0m                 [\n\u001B[1;32m   1196\u001B[0m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtokenizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtokenize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msentence\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_tokenized_string\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1197\u001B[0;31m                     \u001B[0;32mfor\u001B[0m \u001B[0msentence\u001B[0m \u001B[0;32min\u001B[0m \u001B[0msentences\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1198\u001B[0m                 ],\n\u001B[1;32m   1199\u001B[0m                 \u001B[0mkey\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/xrenner/lib/python3.6/site-packages/flair/embeddings/legacy.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m   1195\u001B[0m                 [\n\u001B[1;32m   1196\u001B[0m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtokenizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtokenize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msentence\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_tokenized_string\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1197\u001B[0;31m                     \u001B[0;32mfor\u001B[0m \u001B[0msentence\u001B[0m \u001B[0;32min\u001B[0m \u001B[0msentences\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1198\u001B[0m                 ],\n\u001B[1;32m   1199\u001B[0m                 \u001B[0mkey\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/xrenner/lib/python3.6/site-packages/transformers/tokenization_utils.py\u001B[0m in \u001B[0;36mtokenize\u001B[0;34m(self, text, **kwargs)\u001B[0m\n\u001B[1;32m    350\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    351\u001B[0m         \u001B[0mno_split_token\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munique_no_split_tokens\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 352\u001B[0;31m         \u001B[0mtokenized_text\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msplit_on_tokens\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mno_split_token\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    353\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mtokenized_text\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    354\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/xrenner/lib/python3.6/site-packages/transformers/tokenization_utils.py\u001B[0m in \u001B[0;36msplit_on_tokens\u001B[0;34m(tok_list, text)\u001B[0m\n\u001B[1;32m    344\u001B[0m                     (\n\u001B[1;32m    345\u001B[0m                         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_tokenize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtoken\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mtoken\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munique_no_split_tokens\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mtoken\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 346\u001B[0;31m                         \u001B[0;32mfor\u001B[0m \u001B[0mtoken\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtokenized_text\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    347\u001B[0m                     )\n\u001B[1;32m    348\u001B[0m                 )\n",
      "\u001B[0;32m~/anaconda3/envs/xrenner/lib/python3.6/site-packages/transformers/tokenization_utils.py\u001B[0m in \u001B[0;36m<genexpr>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    344\u001B[0m                     (\n\u001B[1;32m    345\u001B[0m                         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_tokenize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtoken\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mtoken\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munique_no_split_tokens\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mtoken\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 346\u001B[0;31m                         \u001B[0;32mfor\u001B[0m \u001B[0mtoken\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtokenized_text\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    347\u001B[0m                     )\n\u001B[1;32m    348\u001B[0m                 )\n",
      "\u001B[0;32m~/anaconda3/envs/xrenner/lib/python3.6/site-packages/transformers/tokenization_bert.py\u001B[0m in \u001B[0;36m_tokenize\u001B[0;34m(self, text)\u001B[0m\n\u001B[1;32m    222\u001B[0m         \u001B[0msplit_tokens\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    223\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdo_basic_tokenize\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 224\u001B[0;31m             \u001B[0;32mfor\u001B[0m \u001B[0mtoken\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbasic_tokenizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtokenize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnever_split\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mall_special_tokens\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    225\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    226\u001B[0m                 \u001B[0;31m# If the token is part of the never_split set\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/xrenner/lib/python3.6/site-packages/transformers/tokenization_bert.py\u001B[0m in \u001B[0;36mtokenize\u001B[0;34m(self, text, never_split)\u001B[0m\n\u001B[1;32m    411\u001B[0m                     \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstrip_accents\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    412\u001B[0m                         \u001B[0mtoken\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_run_strip_accents\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtoken\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 413\u001B[0;31m                 \u001B[0;32melif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstrip_accents\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    414\u001B[0m                     \u001B[0mtoken\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_run_strip_accents\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtoken\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    415\u001B[0m             \u001B[0msplit_tokens\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mextend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_run_split_on_punc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtoken\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnever_split\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'BasicTokenizer' object has no attribute 'strip_accents'"
     ]
    }
   ],
   "source": [
    "xrenner.analyze(data[0]['conll_text'], out_format='conll')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "xrenner",
   "language": "python",
   "display_name": "xrenner"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}